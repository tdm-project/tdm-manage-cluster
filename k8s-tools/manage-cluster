#!/usr/bin/env bash

# Copyright 2018-2019 CRS4 (http://www.crs4.it/)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#set -x
set -o nounset
set -o errexit
set -o pipefail
# without errtrace functions don't inherit the ERR trap
set -o errtrace

# sw version
VERSION=1.4rc4
# set version of docker images
KsImage="${KsImage:-tdmproject/manage-cluster-ks:${VERSION}}"
TfImage="${TfImage:-tdmproject/manage-cluster-tf:${VERSION}}"
#
InventoryFile=hosts.ini
KubesprayContainerDir=/kubespray

function abspath() {
  local path="${*}"

  if [[ -d "${path}" ]]; then
    echo "$( cd "${path}" >/dev/null && pwd )"
  else
    echo "$( cd "$( dirname "${path}" )" >/dev/null && pwd )/$(basename "${path}")"
  fi
}

function log() {
  echo -e "${@}" >&2
}

function debug_log() {
  if [[ -n "${DEBUG:-}" ]]; then
    echo -e "DEBUG: ${@}" >&2
  fi
}

function error_log() {
  echo -e "ERROR: ${@}" >&2
}

function error_trap() {
  error_log "Error at line ${BASH_LINENO[1]} running the following command:\n\n\t${BASH_COMMAND}\n\n"
  error_log "Stack trace:"
  for (( i=1; i < ${#BASH_SOURCE[@]}; ++i)); do
    error_log "$(printf "%$((4*$i))s %s:%s\n" " " "${BASH_SOURCE[$i]}" "${BASH_LINENO[$i]}")"
  done
  exit 2
}

trap error_trap ERR

function usage_error() {
  if [[ $# > 0 ]]; then
    echo -e "ERROR: ${@}" >&2
  fi
  help
  exit 2
}

function assert_kubectl_installed() {
  if ! command -v kubectl >/dev/null 2>&1 ; then
    error_log "Can't find kubectl executable in PATH.  Kubectl is required to be installed on the client machine"
    exit 2
  fi
}

function print_ostack_vars() {
  for varname in "${!OS_@}"; do
    if [[ "${varname}" == "OS_PASSWORD" ]]; then
      log "   - ${varname}="'**************'
    else
      log "   - ${varname}=${!varname:-}"
    fi
  done
}

function _user_group() {
  echo "$(id -u):$(id -g)"
}

function ansible_verbosity() {
  local av="-v"
  if [[ -n "${DEBUG:-}" ]]; then
    av="-vvvv"
  fi
  echo "${av}"
}

# this function writes a global variable called 'docker_cmdline'
function docker_base_cmd() {
  debug_log "Building docker command"

  docker_cmdline=(docker run -it --rm)

  # pass on env vars that begin with OS_
  debug_log "OpenStack variables:"
  for varname in "${!OS_@}"; do
    # We use the indirect reference syntax ${!varname}
    debug_log "${varname}=${!varname:-}"
    docker_cmdline+=(-e "${varname}=${!varname:-}")
  done

  docker_cmdline+=(-v ${CLUSTER_DIR}/artifacts:${KubesprayContainerDir}/inventory/artifacts)
  docker_cmdline+=(-v ${CLUSTER_DIR}/tf/:${KubesprayContainerDir}/inventory/cluster)
  docker_cmdline+=(-w ${KubesprayContainerDir}/inventory/cluster)

  debug_log "docker_base_cmd: ${docker_cmdline[@]}"
}

function docker_run_tf() {
  debug_log "==== docker_run_tf ===="

  docker_base_cmd  # creates basic docker run cmdline in `docker_cmdline` variable
  docker_cmdline+=(--user $(_user_group))
  docker_cmdline+=("${TfImage}")
  docker_cmdline+=("$@")

  debug_log "${docker_cmdline[@]}"
  "${docker_cmdline[@]}"
}

function docker_run_ks() {
  debug_log "==== docker_run_ks ===="

  docker_base_cmd  # creates basic docker run cmdline in `docker_cmdline` variable
  docker_cmdline+=(--user root)
  docker_cmdline+=("${KsImage}")
  docker_cmdline+=("$@")

  debug_log "${docker_cmdline[@]}"
  "${docker_cmdline[@]}"
}

function gen_template() {
  debug_log "template\nTesting whether ${CLUSTER_DIR} exists"
  if [[ -d "${CLUSTER_DIR}" ]]; then
    usage_error "Can't generate template. ${CLUSTER_DIR} already exists."
  fi

  # cluster name based on config folder name
  local defaultClusterName="$(basename ${CLUSTER_DIR})"
  local defaultSshKeyFile="${defaultClusterName}-ssh-key"

  mkdir -p "${CLUSTER_DIR}"/{artifacts,tf}
  ssh-keygen -N '' -C 'Auto-generated by manage-cluster' -f "${CLUSTER_DIR}/artifacts/${defaultSshKeyFile}"

  # copy sample inventory to CWD (the tf directory in our cluster configuration)
  debug_log "Copying group vars and playbooks from docker container image"
  docker_run_tf /bin/sh -o xtrace -c \
    "cp --dereference --recursive ${KubesprayContainerDir}/contrib/terraform/openstack/sample-inventory/group_vars . \
     && cp --dereference --recursive /home/manageks/config-cluster/* . \
     && mkdir --parents roles"

  debug_log "Customizing template"
  debug_log "Setting cloud provider to openstack, turning on metrics, turning off deployment of helm and LV provisioner"
  sed -i -e 's/^[#[:blank:]]*cloud_provider:.*$/cloud_provider: openstack/' "${CLUSTER_DIR}/tf/group_vars/all/all.yml"
  sed -i-e \
    's/^[#[:blank:]]*metrics_server_enabled:.*$/metrics_server_enabled: true/;
     s/^[#[:blank:]]*helm_enabled:.*$/helm_enabled: false/;
     s/^[#[:blank:]]*local_volume_provisioner_enabled:.*$/local_volume_provisioner_enabled: false/' \
    "${CLUSTER_DIR}/tf/group_vars/k8s-cluster/addons.yml"

  debug_log "Generating cluster.tf template"
  cat > "${CLUSTER_DIR}/tf/cluster.tf" <<END
# your Kubernetes cluster name here
cluster_name = "${defaultClusterName}"
dns_nameservers = ["172.30.3.211", "172.30.3.212"]
# SSH key to use for access to nodes
public_key_path = "../artifacts/${defaultSshKeyFile}.pub"

# image to use for bastion, masters, standalone etcd instances, and nodes
image = "ubuntu-18.04"
# user on the node (ex. core on Container Linux, ubuntu on Ubuntu, etc.)
ssh_user = "ubuntu"

# 0|1 bastion nodes
number_of_bastions = 0
#flavor_bastion = "<UUID>"
flavor_bastion = "0298dfce-aa0f-45d0-91a6-ca8ac2313d94"
#bastion_allowed_remote_ip = ["0.0.0.0/0"]

# standalone etcds
number_of_etcd = 0

# masters
## regular masters are assigned a floating IP
number_of_k8s_masters = 0
## masters_ext_net are masters with a second interface on the external network
## (IP set via DHCP) and no floating IP
number_of_k8s_masters_ext_net = 0
number_of_k8s_masters_no_etcd = 0
number_of_k8s_masters_no_floating_ip = 0
number_of_k8s_masters_no_floating_ip_no_etcd = 0
flavor_k8s_master = "c85e5d87-7b6b-437f-8efe-694782cef1ba"

# nodes
number_of_k8s_nodes = 0
number_of_k8s_nodes_no_floating_ip = 0
flavor_k8s_node = "c85e5d87-7b6b-437f-8efe-694782cef1ba"

# GlusterFS
# either 0 or more than one
#number_of_gfs_nodes_no_floating_ip = 0
#gfs_volume_size_in_gb = 150
# Container Linux does not support GlusterFS
#image_gfs = "<image name>"
# May be different from other nodes
#ssh_user_gfs = "ubuntu"
#flavor_gfs_node = "<UUID>"

# networking
network_name = "${defaultClusterName}-net"
external_net = "2f0db58d-fd9f-4cd8-83fb-59c225a06dc0"
subnet_cidr = "10.99.99.0/24"
floatingip_pool = "external_net"

# scheduling policies
master_vm_scheduler_policy = "soft-anti-affinity"
node_vm_scheduler_policy = "soft-anti-affinity"
END
}

function init() {
  if [ -z ${OS_USERNAME:-} ]; then
    error_log "OS environment variable missing. You should source your OpenStack RC file first.
    Aborting." >&2
    exit 2
  fi

  # avoid using command substitution directly into the string because it won't result
  # in an error if the command fails
  local cluster_name=$(get_cluster_name)
  (printf "\nCurrent cluster '${cluster_name}' (config dir @ '${CLUSTER_DIR}')\n\n" \
    && echo -e "=> Using OpenStack credentials:" && print_ostack_vars \
    && echo -en "\nDo you want to continue (y/n)? ") >&2
  read answer
  echo >&2
  if [[ "${answer}" != y* ]]; then
    exit 0
  fi
}

function deploy_cluster() {
  docker_run_tf terraform init ../../contrib/terraform/openstack
  docker_run_tf terraform apply -auto-approve --var-file=cluster.tf ../../contrib/terraform/openstack
  docker_run_tf /usr/local/bin/create_inventory.py --output "${InventoryFile}"
  log "Now you can run ./manage-cluster deploy-k8s ${CLUSTER_DIR}"
}

function get_cluster_name() {
  awk '!/^ *#/ && /cluster_name *=/ { gsub(/"/, "", $3); print $3; }' ${CLUSTER_DIR}/tf/cluster.tf
}

function get_key_file_name() {
  local public_key=$(awk '!/^ *#/ && /public_key_path *=/ { gsub(/"/, "", $3); print $3; }' ${CLUSTER_DIR}/tf/cluster.tf)
  echo $(basename ${public_key%.pub})
}

function get_key_file_path() {
  local public_key=$(awk '!/^ *#/ && /public_key_path *=/ { gsub(/"/, "", $3); print $3; }' ${CLUSTER_DIR}/tf/cluster.tf)
  echo ${public_key%.pub}
}

function get_ssh_user() {
  awk '!/^ *#/ && /ssh_user *=/ { gsub(/"/, "", $3); print $3; }' ${CLUSTER_DIR}/tf/cluster.tf
}

function get_tf_json() {
  docker_run_tf terraform output -json
}

function _get_master_ips_internal() {
  get_tf_json | jq '.k8s_master_fips.value[]' | tr '"\n' ' '
}

function copy_certs() {
  local master_ip=${1}
  local target_folder="${2}"
  local key_file_path="${CLUSTER_DIR}/artifacts/$(get_key_file_name)"
  local cluster_name="$(get_cluster_name)"
  local ssh_user="$(get_ssh_user)"

  ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i "${key_file_path}" \
      ${ssh_user}@${master_ip} \
      sudo cat "/etc/kubernetes/ssl/apiserver-kubelet-client.key" > "${target_folder}/apiserver-client.key"
  ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i "${key_file_path}" \
      ${ssh_user}@${master_ip} \
      sudo cat "/etc/kubernetes/ssl/apiserver-kubelet-client.crt" > "${target_folder}/apiserver-client.crt"
  ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i "${key_file_path}" \
      ${ssh_user}@${master_ip} \
      sudo cat "/etc/ssl/etcd/ssl/ca.pem" > "${target_folder}/ca.pem"
  chmod go-rwx "${target_folder}"/apiserver-client.{key,crt} "${target_folder}/ca.pem"
}


function deploy_k8s() {
  assert_kubectl_installed

  local keyfile_path="$(get_key_file_path)"
  local cmd=('eval $(ssh-agent -s) ; ' \
             "ssh-add -k '${keyfile_path}' " \
             " && ansible-playbook $(ansible_verbosity) --become -i '${InventoryFile}' --timeout 30 ../../cluster.yml")
  docker_run_ks /bin/sh -o xtrace -c "${cmd[*]}" # expand cmd and compact into a single string

  # generate KubeConfig
  log "Deployment finished.  Configuring client"
  config_client

  log "\nTo finish configuring your cluster run\n\tmanage-cluster config-cluster ${CLUSTER_DIR}"
}

function config_client () {
  assert_kubectl_installed

#FIXME use internal master ip instead of the external one, otherwise ca is invalid
#Current workaround: set skip tls verify
  local targetFolder="${CLUSTER_DIR}/artifacts"
  local clusterName="$(get_cluster_name)"
  local master_ips=( $(_get_master_ips_internal) )

  if [[ ${#master_ips[*]} == 0 ]]; then
    error_log "Couldn't get master IPs from terraform state"
    exit 1
  fi

  debug_log "Got master IPs:  ${master_ips[*]}"

  # copy certs to the "artifacts" folder
  copy_certs ${master_ips[0]} "${targetFolder}"

  debug_log "Generating KubeConfig"
  cat > "${CLUSTER_DIR}/artifacts/kubeconfig" <<KCF
apiVersion: v1
kind: Config
clusters:
contexts:
preferences: {}
users:
KCF
  kubectl config --kubeconfig "${CLUSTER_DIR}/artifacts/kubeconfig" \
    set preferences.colors true >/dev/null

  local username="${clusterName}-admin"
  kubectl config --kubeconfig "${CLUSTER_DIR}/artifacts/kubeconfig" \
    set-credentials "${username}" \
      --client-certificate=${CLUSTER_DIR}/artifacts/apiserver-client.crt \
      --client-key=${CLUSTER_DIR}/artifacts/apiserver-client.key >/dev/null

  local i=0
  for master in "${master_ips[@]}"; do
    debug_log "Adding master ${master} to kubeconfig"
    local master_name=""
    i=$(( $i+1 ))
    if [[ $i > 1 ]]; then
      master_name=$(printf "%s-m%d" "${clusterName}" $i)
    else
      master_name="${clusterName}"
    fi
    kubectl config --kubeconfig "${CLUSTER_DIR}/artifacts/kubeconfig" \
      set-cluster "${master_name}" --server=https://${master}:6443 --insecure-skip-tls-verify=true >/dev/null
    kubectl config --kubeconfig "${CLUSTER_DIR}/artifacts/kubeconfig" \
      set-context "${master_name}" "--cluster=${master_name}" "--user=${username}" >/dev/null
  done

  kubectl config --kubeconfig "${CLUSTER_DIR}/artifacts/kubeconfig" use-context "${clusterName}" >/dev/null

  local sourcing_file="${CLUSTER_DIR}/artifacts/${clusterName}.sh"
  cat > "${sourcing_file}"  <<END

# Source this file to access the kubernetes cluster ${clusterName}
#
# DON'T edit this file!  It will be overwritten if you re-run
# manage-cluster config-client or manage-cluster deploy-k8s.

if [[ "\${SHELL}" == *bash ]]; then
  ArtefactsDir=\$(cd \`dirname \$BASH_SOURCE[0]\` && pwd)
  NewPath="\${ArtefactsDir}/kubeconfig"
  # Prepend the current kubeconfig if it's not already there
  [[ "\${KUBECONFIG}" == \${NewPath}* ]] || KUBECONFIG=\${NewPath}:\${KUBECONFIG}
  unset ArtefactsDir NewPath
else
  # Sorry..if you don't use bash contribute your own path discovery code to manage-cluster
  KUBECONFIG=${CLUSTER_DIR}/artifacts/kubeconfig:\$KUBECONFIG
fi

export KUBECONFIG
echo "KUBECONFIG=\$KUBECONFIG"
echo

random_context=\$(kubectl config get-clusters | sed -e '1d' | sort --random-sort | head -n 1)
kubectl config use-context \${random_context}

# print cluster access points
kubectl cluster-info
END

  export KUBECONFIG="${CLUSTER_DIR}/artifacts/kubeconfig"
  ### From here on we use kubectl with the KUBECONFIG environment variable

  kubectl cluster-info >&2
  log "
  Now you can type:
  export KUBECONFIG=${CLUSTER_DIR}/artifacts/kubeconfig:\$KUBECONFIG
  kubectl config use-context ${clusterName}

  OR, you can source ${sourcing_file}

  If this is a multi-master cluster, remember that you can access it
  through the auxiliary contexts as well (run 'kubectl config get-contexts'
  to print out the list)

  "

  if command -v helm >/dev/null; then
    if ! helm repo add crs4 https://crs4.github.io/helm-charts/ 2>/dev/null ; then
      log "Please initialize helm and add https://crs4.github.io/helm-charts/ to your repositories to install CRS4's charts"
    fi
  fi

  # generate dashboard secrets (token and certificate) for authenticating the dashboard admin user
  _generate_dashboard_admin_user_secrets || true
  log "\n==========================="
  log   "Finished. Client configured"
}

function _generate_dashboard_admin_user_secrets() {
  if [[ -z "${KUBECONFIG:-}" ]]; then
    error_log "BUG!  KUBECONFIG not set when calling _generate_dashboard_admin_user_secrets"
    exit 3
  fi

  local secret_name=$(kubectl get secret -n kube-system  -o custom-columns=NAME:.metadata.name | grep dashboard-admin-user 2>/dev/null)
  if [[ -z "${secret_name}" ]]; then
    error_log "Unable to find the 'dashboard-admin-user' secret on your k8s installation"
    error_log "Maybe the dashboard hasn't been initialized yet"
  else
    local token=$(kubectl get secret -n kube-system ${secret_name} -o jsonpath="{.data.token}")
    if [[ -z "${token}" ]]; then
      error_log "No 'token' found on the 'dashboard-admin-user' secret"
    else
      local token_file="${CLUSTER_DIR}/artifacts/dashboard.token"
      echo "${token}" | base64 --decode > "${token_file}"
      chmod go-rwx "${token_file}"
      log "Kubernetes Dashboard Authentication Token located at '${token_file}'"
    fi
  fi
}

function _run_playbook() {

  local playbook_file="${1}"
  local keyfile_path="$(get_key_file_path)"
  local cmd=('eval $(ssh-agent -s) ; ' \
             "mkdir -p /etc/ansible && " \
             "ln -s /kubespray/roles /etc/ansible/roles ;" \
             "ssh-add -k '${keyfile_path}' && " \
             "ansible-playbook $(ansible_verbosity) --become -i '${InventoryFile}' --timeout 30 '${playbook_file}'")
  docker_run_ks /bin/sh -o xtrace -c "${cmd[*]}" # expand cmd and compact into a single string
}

function config_cluster() {
  _run_playbook config-cluster-playbook.yml
}

function unconfig_cluster() {
  _run_playbook unconfig-cluster-playbook.yml
}

function destroy_cluster(){

  cat >&2 <<END


We are about to deconfigure your cluster (REMOVING PVCs and deployed charts!!!)
This will be done BEFORE Terraform asks you to confirm the destruction of
deployed objects.

===============================================================================

                    ARE YOU SURE YOU WANT TO PROCEED?

===============================================================================

END

  printf "Type \"yes\" to confirm: " >&2
  read answer
  echo >&2
  if [[ $answer != "yes" ]]; then
    log "Aborting destruction"
    exit 0
  fi

  local master_ips=( $(_get_master_ips_internal) )
  # We ignore failures in the unconfig_cluster step, if the deploy-k8s step hasn't been run,
  # The nodes may not have all the required dependencies to execute the playbook
  unconfig_cluster || true
  docker_run_tf terraform init ../../contrib/terraform/openstack;
  docker_run_tf terraform destroy -auto-approve -var-file=cluster.tf ../../contrib/terraform/openstack

  # remove the master IP from the known hosts file, if it's there
  for master in "${master_ips[@]}"; do
    debug_log "Removing local host key for master node ${master}, if present"
    ssh-keygen -R ${master} || true
  done
}

function get_master_ips() {
  local master_ips=( $(_get_master_ips_internal) )
  for master in "${master_ips[@]}"; do
    echo ${master}
  done
}

function open_shell() {
  # Open a shell using the KubeSpray image, which runs as root
  docker_run_ks "/bin/bash"
}

function print_version() {
  echo "${VERSION}"
}

function help() {
  local script_name=$(basename "$0")
  echo -e "\nUsage of '${script_name}'

  ${script_name} <COMMAND> [options] <CLUSTER_DIR>
  ${script_name} -h        prints this help message
  ${script_name} -v        prints the '${script_name}' version

  COMMAND:
    template       creates a template cluster configuration directory
    deploy         creates virtual machines
    deploy-k8s     deploys kubernetes
    config-cluster run ansible playbook to configure the kubernetes cluster. The default
                     playbook contains CRS4-specific customizations.
    destroy        destroys virtual machines
    config-client  configures kubectl
    get-master-ips prints out master IPs for the cluster, one per line (cluster must be deployed)
    shell          opens a shell in the manage-cluster container

  CLUSTER_DIR:
    Path to the directory containing the cluster's configuration
    (i.e., terraform files and artifacts)

    For details about what manage-cluster does, check out
    https://github.com/kubernetes-incubator/kubespray/tree/master/contrib/terraform/openstack
  " >&2
}

############## main #################

if [[ $# -lt 1 ]]; then
  usage_error
fi

while getopts ":h :v" opt; do
  case "$opt" in
      h) help; exit 0 ;;
      v) print_version; exit 0 ;;
      \?)
        usage_error "Invalid option: -$OPTARG"
        ;;
      :)
        usage_error "Option -$OPTARG requires an argument"
        ;;
  esac
done

if [[ $# != 2 ]]; then
  # At this point, we always need at two arguments from the user: subcommand and path
  usage_error
fi

# shift arguments forward by the amount processed by getopts
shift $(($OPTIND - 1))

SkipInit=false
COMMAND="${1}"
shift

debug_log "COMMAND: ${COMMAND}"

case "$COMMAND" in
  deploy)
    FUNCTION=deploy_cluster
    ;;
  deploy-k8s)
    FUNCTION=deploy_k8s
    ;;
  config-cluster)
    FUNCTION=config_cluster
    ;;
  destroy)
    FUNCTION=destroy_cluster
    ;;
  config-client)
    SkipInit=true
    FUNCTION=config_client
    ;;
  get-master-ips)
    SkipInit=true
    FUNCTION=get_master_ips
    ;;
  shell)
    FUNCTION=open_shell
    ;;
  template)
    SkipInit=true
    FUNCTION=gen_template
    ;;
  *)
    usage_error "Command \"$COMMAND\" not found. "
    ;;
esac

# CLUSTER_DIR is the directory that contains our cluster's configuration.
# Create one in the required format with the gen-template command.
CLUSTER_DIR=$(abspath "${1}")
debug_log "CLUSTER_DIR: ${CLUSTER_DIR}"

debug_log "Existing traps: $(trap -p)"

if [[ "${FUNCTION}" != gen_template ]]; then
  if [[ ! -d "${CLUSTER_DIR}" ]]; then
    usage_error "Cluster configuration directory ${CLUSTER_DIR} doens't exist"
  fi
fi

if [[ "${SkipInit}" != "true" ]]; then
  init
else
  debug_log "Skipping init function for command ${FUNCTION}"
fi

$FUNCTION

